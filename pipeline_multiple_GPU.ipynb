{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "415aa11b-f254-47c3-946a-d3bc1b99c72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [02:03<00:00, 61.82s/it]\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:04<00:00,  2.28s/it]\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.36s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "access_token = \"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\"\n",
    "device_list = [\n",
    "    'cuda:0',\n",
    "               'cuda:1'\n",
    "               # 'cuda:2',\n",
    "               # 'cuda:3',\n",
    "               # 'cuda:4'\n",
    "               # 'cuda:5',\n",
    "               # 'cuda:6'\n",
    "]\n",
    "torch_device_list = []\n",
    "for i in device_list:\n",
    "    torch_device_list.append(torch.device(i))\n",
    "\n",
    "# Load the model and tokenizer\n",
    "# model_name = \"meta-llama/CodeLlama-7b-hf\"\n",
    "# model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# model_name = \"huggyllama/llama-7b\"\n",
    "model_name = 'meta-llama/Llama-2-7b-hf'\n",
    "\n",
    "pipelines = []\n",
    "for i in torch_device_list:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name , token=access_token)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name , token=access_token).to(i)\n",
    "    \n",
    "    pipeline = transformers.pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        torch_dtype=torch.float16,\n",
    "        device = i,#\"auto\",\n",
    "        tokenizer=tokenizer)\n",
    "    pipelines.append( pipeline )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeeb5677-d8f6-4eb5-89ad-0c77024923e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "def extract_prompt_from_list_of_questions(question_list):\n",
    "    batch_list = []\n",
    "    for i in question_list:\n",
    "        batch_list.append(i['prompt'])\n",
    "    return batch_list\n",
    "    \n",
    "def generate_answer_from_LLM(prompt_list , pipeline):\n",
    "    sequences = pipeline(\n",
    "        prompt_list,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        temperature=0.1,\n",
    "        top_p=0.95,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        # eos_token_id=tokenizer.convert_tokens_to_ids(';'),\n",
    "        max_new_tokens=200,\n",
    "    )\n",
    "    return sequences\n",
    "\n",
    "from utils.post_process import get_exec_output\n",
    "db_dir = './DAIL-SQL/dataset/spider/database'\n",
    "# import asyncio\n",
    "def query_to_db(query , db_id):\n",
    "    db_path = f\"{db_dir}/{db_id}/{db_id}\"\n",
    "    flag, denotation = get_exec_output(\n",
    "            db_path,\n",
    "            query)\n",
    "    return flag, denotation\n",
    "\n",
    "from utils.post_process import result_eq\n",
    "def put_responses_back_to_json_dataset(index , json_dataset , sequences):\n",
    "    execution_accuracy = 0\n",
    "    for i , seq in enumerate(sequences):\n",
    "        prompt_len = len ( json_dataset['questions'][index+i]['prompt'] )\n",
    "        qustion = json_dataset['questions'][index+i]['prompt'].splitlines()[-2]\n",
    "        start_of_answer = json_dataset['questions'][index+i]['prompt'].splitlines()[-1]\n",
    "        ground_truth = start_of_answer + ' ' + json_dataset['questions'][index+i]['response']\n",
    "        gen_text = seq[0]['generated_text'][prompt_len:]\n",
    "        processed_gen_text = post_process_get_sql_from_gentext(gen_text)\n",
    "        # print('Question: ' , qustion )\n",
    "        # print('GroundTruth answer: ' ,  ground_truth )\n",
    "        # print('Generated answer: ' , gen_text )\n",
    "        # print('processed_gen_text: ' , processed_gen_text )\n",
    "        db_id = json_dataset['questions'][index+i]['db_id']\n",
    "        flag1, denotation1 = query_to_db(processed_gen_text , db_id) #flag has ('result' , [data in columns])\n",
    "        flag2, denotation2 = query_to_db(ground_truth , db_id)\n",
    "        if flag1[0] != 'result':\n",
    "            is_equal = False\n",
    "            # print(flag1[0] , ' --> ' , 'processed_gen_text: ' , processed_gen_text )\n",
    "        elif 'ORDER BY' in ground_truth or 'order by' in ground_truth:\n",
    "            is_equal = result_eq(flag1[1] , flag2[1] , order_matters=True)\n",
    "        else:\n",
    "            is_equal = result_eq(flag1[1] , flag2[1] , order_matters=False)\n",
    "        json_dataset['questions'][index+i]['response'] = processed_gen_text\n",
    "        execution_accuracy += is_equal\n",
    "        # print('is_equal: ', is_equal)\n",
    "        # print('--------------------------')\n",
    "    return execution_accuracy\n",
    "\n",
    "from utils.post_process import process_duplication\n",
    "def post_process_get_sql_from_gentext(gen_text):\n",
    "    # remove \\n and extra spaces\n",
    "    sql = \" \".join(gen_text.replace(\"\\n\", \" \").split())\n",
    "    sql = process_duplication(sql)\n",
    "    # python version should >= 3.8\n",
    "    if sql.startswith(\"SELECT\"):\n",
    "        sql = sql\n",
    "    elif sql.startswith(\" \"):\n",
    "        sql = \"SELECT\" + sql\n",
    "    else:\n",
    "        sql = \"SELECT \" + sql\n",
    "    return sql\n",
    "\n",
    "def process_a_batch(question_units, pipeline, pipeline_sequences ,pipeline_index):\n",
    "    start_time = time.time()\n",
    "    batch_list = extract_prompt_from_list_of_questions(question_units)\n",
    "    sequences = generate_answer_from_LLM(batch_list , pipeline)\n",
    "    pipeline_sequences.extend(sequences)\n",
    "    # print(f\"{pipeline_index}: batch_time: \", time.time() - start_time )\n",
    "    return pipeline_sequences\n",
    "    \n",
    "def pipline_process(pipeline , data , batch_size, pipeline_index):\n",
    "    start_time = time.time()\n",
    "    pipeline_sequences = []\n",
    "    chunk_size = len(data)\n",
    "    number_of_batches = int(chunk_size/batch_size)\n",
    "    print(f'{pipeline_index}: Number of batches for this GPU:' , number_of_batches )\n",
    "    for index in range( number_of_batches ):\n",
    "        if index == 1:\n",
    "            print(f\"{pipeline_index}: Processing time for the first batch: {time.time() - start_time} \\n\")\n",
    "        question_units = data[ index*batch_size : index*batch_size+batch_size ]\n",
    "        pipeline_sequences = process_a_batch(question_units, pipeline, pipeline_sequences ,pipeline_index)\n",
    "        if ( index*batch_size + batch_size < chunk_size ) and ( index*batch_size + 2*batch_size > chunk_size ):\n",
    "            question_units = data[index*batch_size + batch_size :]\n",
    "            pipeline_sequences = process_a_batch(question_units, pipeline, pipeline_sequences, pipeline_index)\n",
    "    with open('./log.txt', 'a') as f:\n",
    "        f.write(f\"{pipeline_index}: pipeline_process_time: {time.time() - start_time} \\n\")\n",
    "    # print('pipeline_process_time: ', time.time() - start_time )\n",
    "    return pipeline_sequences\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0aba56-e2e9-4b7e-ba2b-6bde7526799a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Number of batches for this GPU: 47\n",
      "1: Number of batches for this GPU: 47\n",
      "1: Processing time for the first batch: 177.83168196678162 \n",
      "\n",
      "0: Processing time for the first batch: 196.3959460258484 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    }
   ],
   "source": [
    "with open('./DAIL-SQL/dataset/process/BIRD-TEST_SQL_5-SHOT_EUCDISMASKPRESKLSIMTHR_QA-EXAMPLE_CTX-200_ANS-2048/questions.json' , 'r') as f:\n",
    "    generated_prompts_file_byte = f.read()\n",
    "    generated_prompts = json.loads(generated_prompts_file_byte)\n",
    "\n",
    "from threading import Thread\n",
    "class ThreadWithReturnValue(Thread):\n",
    "    \n",
    "    def __init__(self, group=None, target=None, name=None,\n",
    "                 args=(), kwargs={}, Verbose=None):\n",
    "        Thread.__init__(self, group, target, name, args, kwargs)\n",
    "        self._return = None\n",
    "\n",
    "    def run(self):\n",
    "        if self._target is not None:\n",
    "            self._return = self._target(*self._args,\n",
    "                                                **self._kwargs)\n",
    "    def join(self, *args):\n",
    "        Thread.join(self, *args)\n",
    "        return self._return\n",
    "\n",
    "batch_size = 16\n",
    "data_size = len( generated_prompts['questions'] )\n",
    "number_of_GPUs = len(pipelines)\n",
    "avg_chunk_size = int(data_size/number_of_GPUs)\n",
    "data_chunks = [ generated_prompts['questions'][avg_chunk_size*i : avg_chunk_size*i+avg_chunk_size] for i in range( number_of_GPUs )] #divides the data into chunkes for each GPU\n",
    "if data_size%number_of_GPUs!=0:\n",
    "    data_chunks[-1].extend(generated_prompts['questions'][ number_of_GPUs * avg_chunk_size : data_size ])\n",
    "output_sequences = []\n",
    "pipeline_threads = []\n",
    "for pipeline_index , pipeline in enumerate(pipelines):\n",
    "    data = data_chunks[pipeline_index]\n",
    "    pipeline_thread = ThreadWithReturnValue(target= pipline_process , args = (pipeline , data , batch_size, pipeline_index) )\n",
    "    pipeline_threads.append(pipeline_thread)\n",
    "    pipeline_thread.start()\n",
    "\n",
    "for pipeline_thread in pipeline_threads:\n",
    "    pipeline_sequences = pipeline_thread.join()\n",
    "    output_sequences.extend(pipeline_sequences)\n",
    "    \n",
    "# print(output_sequences)\n",
    "# print('execution accuracy = ' , exec_acc/data_size)\n",
    "# with open('./llama_pred/SPIDER-TEST_SQL_0-SHOT_CTX-200_ANS-2048.json' , 'w' )as f:\n",
    "#     json.dump(generated_prompts , f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0e13480-0719-439a-b953-06559638d67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1534\n"
     ]
    }
   ],
   "source": [
    "print(len(output_sequences))\n",
    "import pickle\n",
    "with open('output_sequences.pkl', 'wb') as f:  # open a text file\n",
    "    pickle.dump( output_sequences , f )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79f730e6-c0d8-4030-82d3-714e40aee72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/* Some SQL examples are provided based on similar problems: */\n",
      "/* Answer the following: What is the highest infant mortality rate per thousand of the countries whose inflation is under 3? */\n",
      "SELECT MAX(T2.Infant_Mortality) FROM economy AS T1 INNER JOIN population AS T2 ON T1.Country = T2.Country WHERE T1.Inflation < 3\n",
      "\n",
      "/* Answer the following: List at least 5 students who has the longest absense from schoool? longest absense refers to MAX(month) */\n",
      "SELECT name FROM longest_absense_from_school ORDER BY month DESC LIMIT 5\n",
      "\n",
      "/* Answer the following: What is the number of unemployed and bankrupt students? */\n",
      "SELECT COUNT(T1.name) FROM unemployed AS T1 INNER JOIN filed_for_bankrupcy AS T2 ON T1.name = T2.name\n",
      "\n",
      "/* Answer the following: What is the keyword for episodes with stars score of 10 at 30% and above? stars score of 10 at 30% and above refers to stars = 10 and percent > 29 */\n",
      "SELECT T1.keyword FROM Keyword AS T1 INNER JOIN Vote AS T2 ON T1.episode_id = T2.episode_id WHERE T2.stars = 10 AND T2.percent > 29;\n",
      "\n",
      "/* Answer the following: Tell the number of 4-year public schools in UT whose graduation rate exceeds the average for the state. 4-year refers to level = '4-year'; public refers to control = 'Public'; UT refers to state_abbr = 'UT'; graduation rate exceeds the average for the state refers to awards_per_value > awards_per_state_value; */\n",
      "SELECT COUNT(DISTINCT T1.chronname) FROM institution_details AS T1 INNER JOIN state_sector_grads AS T2 ON T2.state = T1.state WHERE T2.state_abbr = 'UT' AND T1.level = '4-year' AND T1.control = 'Public' AND T1.awards_per_value > T1.awards_per_state_value\n",
      "\n",
      "/* Given the following database schema: */\n",
      "CREATE TABLE frpm\n",
      "(\n",
      "    CDSCode                                       TEXT not null\n",
      "        primary key,\n",
      "    `Academic Year`                               TEXT  null,\n",
      "    `County Code`                                 TEXT  null,\n",
      "    `District Code`                               INTEGER         null,\n",
      "    `School Code`                                 TEXT  null,\n",
      "    `County Name`                                 TEXT null,\n",
      "    `District Name`                               TEXT null,\n",
      "    `School Name`                                 TEXT null,\n",
      "    `District Type`                               TEXT null,\n",
      "    `School Type`                                 TEXT null,\n",
      "    `Educational Option Type`                     TEXT null,\n",
      "    `NSLP Provision Status`                       TEXT null,\n",
      "    `Charter School (Y/N)`                        INTEGER    null,\n",
      "    `Charter School Number`                       TEXT  null,\n",
      "    `Charter Funding Type`                        TEXT null,\n",
      "    IRC                                           INTEGER    null,\n",
      "    `Low Grade`                                   TEXT  null,\n",
      "    `High Grade`                                  TEXT null,\n",
      "    `Enrollment (K-12)`                           REAL      null,\n",
      "    `Free Meal Count (K-12)`                      REAL       null,\n",
      "    `Percent (%) Eligible Free (K-12)`            REAL       null,\n",
      "    `FRPM Count (K-12)`                           REAL       null,\n",
      "    `Percent (%) Eligible FRPM (K-12)`            REAL       null,\n",
      "    `Enrollment (Ages 5-17)`                      REAL       null,\n",
      "    `Free Meal Count (Ages 5-17)`                 REAL       null,\n",
      "    `Percent (%) Eligible Free (Ages 5-17)`       REAL       null,\n",
      "    `FRPM Count (Ages 5-17)`                      REAL       null,\n",
      "    `Percent (%) Eligible FRPM (Ages 5-17)`       REAL       null,\n",
      "    `2013-14 CALPADS Fall 1 Certification Status` INTEGER    null,\n",
      "    foreign key (CDSCode) references schools (CDSCode)\n",
      ")\n",
      "\n",
      "CREATE TABLE satscores\n",
      "(\n",
      "    cds         TEXT not null\n",
      "        primary key,\n",
      "    rtype       TEXT  not null,\n",
      "    sname       TEXT null,\n",
      "    dname       TEXT null,\n",
      "    cname       TEXT null,\n",
      "    enroll12    INTEGER         not null,\n",
      "    NumTstTakr  INTEGER          not null,\n",
      "    AvgScrRead  INTEGER          null,\n",
      "    AvgScrMath  INTEGER          null,\n",
      "    AvgScrWrite INTEGER          null,\n",
      "    NumGE1500   INTEGER          null,\n",
      "--     PctGE1500   double      null,\n",
      "        foreign key (cds) references schools (CDSCode)\n",
      ")\n",
      "\n",
      "CREATE TABLE schools\n",
      "(\n",
      "    CDSCode     TEXT not null\n",
      "        primary key,\n",
      "    NCESDist    TEXT  null,\n",
      "    NCESSchool  TEXT  null,\n",
      "    StatusType  TEXT  not null,\n",
      "    County      TEXT not null,\n",
      "    District    TEXT not null,\n",
      "    School      TEXT null,\n",
      "    Street      TEXT null,\n",
      "    StreetAbr   TEXT null,\n",
      "    City        TEXT null,\n",
      "    Zip         TEXT null,\n",
      "    State       TEXT  null,\n",
      "    MailStreet  TEXT null,\n",
      "    MailStrAbr  TEXT null,\n",
      "    MailCity    TEXT null,\n",
      "    MailZip     TEXT null,\n",
      "    MailState   TEXT  null,\n",
      "    Phone       TEXT null,\n",
      "    Ext         TEXT  null,\n",
      "    Website     TEXT null,\n",
      "    OpenDate    DATE        null,\n",
      "    ClosedDate  DATE        null,\n",
      "    Charter     INTEGER    null,\n",
      "    CharterNum  TEXT  null,\n",
      "    FundingType TEXT null,\n",
      "    DOC         TEXT  not null,\n",
      "    DOCType     TEXT not null,\n",
      "    SOC         TEXT  null,\n",
      "    SOCType     TEXT null,\n",
      "    EdOpsCode   TEXT  null,\n",
      "    EdOpsName   TEXT null,\n",
      "    EILCode     TEXT  null,\n",
      "    EILName     TEXT null,\n",
      "    GSoffered   TEXT null,\n",
      "    GSserved    TEXT  null,\n",
      "    Virtual     TEXT  null,\n",
      "    Magnet      INTEGER   null,\n",
      "    Latitude    REAL      null,\n",
      "    Longitude   REAL      null,\n",
      "    AdmFName1   TEXT null,\n",
      "    AdmLName1   TEXT null,\n",
      "    AdmEmail1   TEXT null,\n",
      "    AdmFName2   TEXT null,\n",
      "    AdmLName2   TEXT null,\n",
      "    AdmEmail2   TEXT null,\n",
      "    AdmFName3   TEXT  null,\n",
      "    AdmLName3   TEXT null,\n",
      "    AdmEmail3   TEXT null,\n",
      "    LastUpdate  DATE        not null\n",
      ")\n",
      "\n",
      "/* Answer the following: What is the highest eligible free rate for K-12 students in the schools in Alameda County? Eligible free rate for K-12 = `Free Meal Count (K-12)` / `Enrollment (K-12)` */\n",
      "SELECT  T1.cds,\n",
      "        T2.rtype,\n",
      "        T3.sname,\n",
      "        T4.dname,\n",
      "        T5.cname,\n",
      "        T6.enroll12,\n",
      "        T7.NumTstTakr,\n",
      "        T8.AvgScrRead,\n",
      "        T9.AvgScrMath,\n",
      "        T10.AvgScrWrite,\n",
      "        T11.NumGE1500,\n",
      "        T12.PctGE1500,\n",
      "        T13.cds\n",
      "FROM    schools AS T1\n",
      "        INNER JOIN satscores AS T2 ON T1.cds = T2.cds\n",
      "        INNER JOIN schools AS T3 ON T2.cds = T3.cds\n",
      "        INNER JOIN satscores AS T4 ON T3.cds = T4.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('output_sequences.pkl', 'rb') as f:  # open a text file\n",
    "    output_sequences = pickle.load(f)\n",
    "\n",
    "print(output_sequences[0][0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef99a4b1-7d5d-44f8-9215-cf9ac5481ab7",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m     generated_prompts \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(generated_prompts_file_byte)\n\u001b[1;32m      6\u001b[0m data_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(generated_prompts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestions\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 8\u001b[0m exec_acc \u001b[38;5;241m=\u001b[39m \u001b[43mput_responses_back_to_json_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerated_prompts\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_sequences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexecution accuracy = \u001b[39m\u001b[38;5;124m'\u001b[39m , exec_acc\u001b[38;5;241m/\u001b[39mdata_size)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./llama_pred/SPIDER-TEST_SQL_0-SHOT_CTX-200_ANS-2048.json\u001b[39m\u001b[38;5;124m'\u001b[39m , \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m )\u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "Cell \u001b[0;32mIn[1], line 37\u001b[0m, in \u001b[0;36mput_responses_back_to_json_dataset\u001b[0;34m(index, json_dataset, sequences)\u001b[0m\n\u001b[1;32m     35\u001b[0m execution_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i , seq \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sequences):\n\u001b[0;32m---> 37\u001b[0m     prompt_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m ( \u001b[43mjson_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquestions\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m'\u001b[39m] )\n\u001b[1;32m     38\u001b[0m     qustion \u001b[38;5;241m=\u001b[39m json_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestions\u001b[39m\u001b[38;5;124m'\u001b[39m][index\u001b[38;5;241m+\u001b[39mi][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msplitlines()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     39\u001b[0m     start_of_answer \u001b[38;5;241m=\u001b[39m json_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestions\u001b[39m\u001b[38;5;124m'\u001b[39m][index\u001b[38;5;241m+\u001b[39mi][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msplitlines()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('./DAIL-SQL/dataset/process/SPIDER-TEST_SQL_0-SHOT_CTX-200_ANS-2048/questions.json' , 'r') as f:\n",
    "    generated_prompts_file_byte = f.read()\n",
    "    generated_prompts = json.loads(generated_prompts_file_byte)\n",
    "\n",
    "data_size = len(generated_prompts['questions'])\n",
    "\n",
    "exec_acc = put_responses_back_to_json_dataset(0 , generated_prompts , output_sequences)\n",
    "\n",
    "print('execution accuracy = ' , exec_acc/data_size)\n",
    "with open('./llama_pred/SPIDER-TEST_SQL_0-SHOT_CTX-200_ANS-2048.json' , 'w' )as f:\n",
    "    json.dump(generated_prompts , f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c2aa42d-3bb7-4d60-9f5f-49032932a072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('output_sequences.pkl', 'rb') as f:  # open a text file\n",
    "    output_sequences = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279f3c09-85f5-4def-bbed-8d1d24767afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/yadegari_llama/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "Downloading shards:  57%|██████████████████████████████████████████████████████▎                                        | 4/7 [06:09<04:37, 92.44s/it]"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import gather_object\n",
    "import os\n",
    "\n",
    "#The method to run an inference model that fits in a single GPU on a given dataset using multiple GPUs.\n",
    "\n",
    "#access_token and model_name for Llama-2-7b-CHAT\n",
    "access_token = \"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\"\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "#model for vicuna-33b\n",
    "access_token = \"\"\n",
    "model_name = \"lmsys/vicuna-33b-v1.3\"\n",
    "\n",
    "accelerator = Accelerator()\n",
    "if access_token =='':\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name )\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name ,device_map={\"\": 0,\n",
    "                                                                       \"\": 2,\n",
    "                                                                       \"\": 3,\n",
    "                                                                       \"\": 4,\n",
    "                                                                       \"\": 5,\n",
    "                 \"\": 6} )\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name , token=access_token)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name , token=access_token , device_map={\"\": 0,\n",
    "                                                                                               \"\": 2,\n",
    "                                                                                               \"\": 3,\n",
    "                                                                                               \"\": 4,\n",
    "                                                                                               \"\": 5,\n",
    "                 \"\": 6} )\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map = 'balanced',\n",
    "    tokenizer=tokenizer )\n",
    "\n",
    "\n",
    "def extract_prompt_from_list_of_questions(question_list):\n",
    "    batch_list = []\n",
    "    for i in question_list:\n",
    "        batch_list.append(i['prompt'])\n",
    "    return batch_list\n",
    "\n",
    "def generate_answer_from_LLM(prompt_list , pipeline):\n",
    "    sequences = pipeline(\n",
    "        prompt_list,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        temperature=0.1,\n",
    "        top_p=0.95,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        # eos_token_id=tokenizer.convert_tokens_to_ids(';'),\n",
    "        max_new_tokens=200,\n",
    "    )\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1932c0f-1be6-46fc-b1b4-99130341bad3",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:3 and cuda:6! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m question_units \u001b[38;5;241m=\u001b[39m per_GPU_chunks[i:i\u001b[38;5;241m+\u001b[39mbatch_size]\n\u001b[1;32m     20\u001b[0m batch_list \u001b[38;5;241m=\u001b[39m extract_prompt_from_list_of_questions(question_units)\n\u001b[0;32m---> 21\u001b[0m sequences \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_answer_from_LLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_list\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_sequences\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mextend(sequences)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ( i \u001b[38;5;241m+\u001b[39m batch_size \u001b[38;5;241m<\u001b[39m chunk_size ) \u001b[38;5;129;01mand\u001b[39;00m ( i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mbatch_size \u001b[38;5;241m>\u001b[39m chunk_size ):\n",
      "Cell \u001b[0;32mIn[1], line 43\u001b[0m, in \u001b[0;36mgenerate_answer_from_LLM\u001b[0;34m(prompt_list, pipeline)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_answer_from_LLM\u001b[39m(prompt_list , pipeline):\n\u001b[0;32m---> 43\u001b[0m     sequences \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# eos_token_id=tokenizer.convert_tokens_to_ids(';'),\u001b[39;49;00m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sequences\n",
      "File \u001b[0;32m/opt/anaconda3/envs/yadegari_llama/lib/python3.12/site-packages/transformers/pipelines/text_generation.py:262\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(chats, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/yadegari_llama/lib/python3.12/site-packages/transformers/pipelines/base.py:1235\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1232\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   1233\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1234\u001b[0m     )\n\u001b[0;32m-> 1235\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1236\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[1;32m   1237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/yadegari_llama/lib/python3.12/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/yadegari_llama/lib/python3.12/site-packages/transformers/pipelines/pt_utils.py:125\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m    124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[0;32m--> 125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;66;03m# Try to infer the size of the batch\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/yadegari_llama/lib/python3.12/site-packages/transformers/pipelines/base.py:1161\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1159\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1160\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1161\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1162\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/yadegari_llama/lib/python3.12/site-packages/transformers/pipelines/text_generation.py:349\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    346\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prefix_length\n\u001b[1;32m    348\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 349\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    350\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/yadegari_llama/lib/python3.12/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/yadegari_llama/lib/python3.12/site-packages/transformers/generation/utils.py:1914\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1906\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1907\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1908\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1909\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1910\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1911\u001b[0m     )\n\u001b[1;32m   1913\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 1914\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1915\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1916\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1917\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1922\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1923\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   1926\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1927\u001b[0m     prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1928\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   1929\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample\n\u001b[1;32m   1930\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1931\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/yadegari_llama/lib/python3.12/site-packages/transformers/generation/utils.py:2651\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2648\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2650\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2651\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2652\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2653\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2654\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2655\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2656\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2658\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2659\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/yadegari_llama/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/yadegari_llama/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/yadegari_llama/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:1174\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1171\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1174\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1187\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/yadegari_llama/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/yadegari_llama/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/yadegari_llama/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:931\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    928\u001b[0m     use_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    933\u001b[0m return_legacy_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(past_key_values, Cache):  \u001b[38;5;66;03m# kept for BC (non `Cache` `past_key_values` inputs)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/yadegari_llama/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/yadegari_llama/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/yadegari_llama/lib/python3.12/site-packages/torch/nn/modules/sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/yadegari_llama/lib/python3.12/site-packages/torch/nn/functional.py:2264\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2258\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2259\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2260\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2261\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2262\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2263\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:3 and cuda:6! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "with open('./DAIL-SQL/dataset/process/SPIDER-TEST_SQL_0-SHOT_CTX-200_ANS-2048/questions.json' , 'r') as f:\n",
    "    generated_prompts_file_byte = f.read()\n",
    "    generated_prompts = json.loads(generated_prompts_file_byte)\n",
    "\n",
    "batch_size = 2\n",
    "exec_acc = 0\n",
    "data_size = len( generated_prompts['questions'] )\n",
    "\n",
    "\n",
    "accelerator.wait_for_everyone()    \n",
    "start=time.time()\n",
    "# print(pipeline.hf_device_map)\n",
    "with accelerator.split_between_processes(generated_prompts['questions'][:10]) as per_GPU_chunks:\n",
    "    results = dict(output_sequences =[])\n",
    "    chunk_size = len(per_GPU_chunks)\n",
    "    for i in range( 0 , chunk_size , batch_size ):\n",
    "        question_units = per_GPU_chunks[i:i+batch_size]\n",
    "        batch_list = extract_prompt_from_list_of_questions(question_units)\n",
    "        sequences = generate_answer_from_LLM(batch_list , pipeline)\n",
    "        results['output_sequences'].extend(sequences)\n",
    "            \n",
    "        if ( i + batch_size < chunk_size ) and ( i + 2*batch_size > chunk_size ):\n",
    "            question_units = generated_prompts['questions'][i + batch_size :]\n",
    "            batch_list = extract_prompt_from_list_of_questions(question_units)\n",
    "            sequences = generate_answer_from_LLM(batch_list , pipeline)\n",
    "            results['output_sequences'].extend(sequences)\n",
    "    results = [ results ]\n",
    "results_gathered = gather_object(results)\n",
    "if accelerator.is_main_process:\n",
    "    timediff=time.time()-start\n",
    "    print('timediff: ' , timediff)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "571194bd-544f-4391-877e-5f12a9828b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'': 0}\n"
     ]
    }
   ],
   "source": [
    "print({\"\": accelerator.process_index})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f80cd1d7-fa70-4968-8f5f-bfd622c78dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "<class 'torch.device'>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from accelerate import PartialState\n",
    "distributed_state = PartialState()\n",
    "print(distributed_state.device)\n",
    "print(type(distributed_state.device))\n",
    "print(distributed_state.device.index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
