{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e271cbbe-e42d-4e30-8cbc-75a0442fb470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# with open('./DAIL-SQL/dataset/process/SPIDER-TEST_SQL_0-SHOT_CTX-200_ANS-2048/questions.json' , 'r') as f:\n",
    "#     generated_prompts_file_byte = f.read()\n",
    "#     generated_prompts = json.loads(generated_prompts_file_byte)\n",
    "\n",
    "with open('./llama_pred/BIRD-TEST_SQL_0-SHOT_CTX-200_ANS-2048_Llama_7b.json' , 'r') as f:\n",
    "    generated_response_file_byte = f.read()\n",
    "    generated_response = json.loads(generated_response_file_byte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc89654e-3f05-4532-a0d2-cf86cd8ac075",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./llama_pred/BIRD-TEST_SQL_0-SHOT_CTX-200_ANS-2048_Llama_7b.txt' , 'w')as f:\n",
    "    for i in generated_response['questions']:\n",
    "        f.write( i['response']+'\\n' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "899ffe40-d041-4327-92b3-efd14c9409b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('output_sequences.pkl', 'rb') as f:  # open a text file\n",
    "    output_sequences = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f60b7b7-62e4-4293-8cae-ece743cdd1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "def extract_prompt_from_list_of_questions(question_list):\n",
    "    batch_list = []\n",
    "    for i in question_list:\n",
    "        batch_list.append(i['prompt'])\n",
    "    return batch_list\n",
    "    \n",
    "def generate_answer_from_LLM(prompt_list , pipeline):\n",
    "    sequences = pipeline(\n",
    "        prompt_list,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        temperature=0.1,\n",
    "        top_p=0.95,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        # eos_token_id=tokenizer.convert_tokens_to_ids(';'),\n",
    "        max_new_tokens=200,\n",
    "    )\n",
    "    return sequences\n",
    "\n",
    "from utils.post_process import get_exec_output\n",
    "\n",
    "# import asyncio\n",
    "def query_to_db(query , db_id , db_dir):\n",
    "    db_path = f\"{db_dir}/{db_id}/{db_id}\"\n",
    "    flag, denotation = get_exec_output(\n",
    "            db_path,\n",
    "            query)\n",
    "    return flag, denotation\n",
    "\n",
    "from utils.post_process import result_eq\n",
    "def put_responses_back_to_json_dataset(index , json_dataset , sequences, dataset_type='spider'):#dataset_type=spider/bird\n",
    "    db_dir = './DAIL-SQL/dataset/'+ dataset_type +'/database'\n",
    "    execution_accuracy = 0\n",
    "    for i , seq in enumerate(sequences):\n",
    "        prompt_len = len ( json_dataset['questions'][index+i]['prompt'] )\n",
    "        qustion = json_dataset['questions'][index+i]['prompt'].splitlines()[-2]\n",
    "        start_of_answer = json_dataset['questions'][index+i]['prompt'].splitlines()[-1]\n",
    "        ground_truth = start_of_answer + ' ' + json_dataset['questions'][index+i]['response']\n",
    "        gen_text = seq[0]['generated_text'][prompt_len:]\n",
    "        processed_gen_text = post_process_get_sql_from_gentext(gen_text)\n",
    "        # print('Question: ' , qustion )\n",
    "        # print('GroundTruth answer: ' ,  ground_truth )\n",
    "        # print('Generated answer: ' , gen_text )\n",
    "        # print('processed_gen_text: ' , processed_gen_text )\n",
    "        db_id = json_dataset['questions'][index+i]['db_id']\n",
    "        flag1, denotation1 = query_to_db(processed_gen_text , db_id , db_dir) #flag has ('result' , [data in columns])\n",
    "        flag2, denotation2 = query_to_db(ground_truth , db_id , db_dir)\n",
    "        if flag2[0] != 'result':\n",
    "            print('The following ground truth has an error:' , ground_truth)\n",
    "            is_equal = False\n",
    "        elif flag1[0] != 'result':\n",
    "            is_equal = False\n",
    "            # print(flag1[0] , ' --> ' , 'processed_gen_text: ' , processed_gen_text )\n",
    "        elif 'ORDER BY' in ground_truth or 'order by' in ground_truth:\n",
    "            is_equal = result_eq(flag1[1] , flag2[1] , order_matters=True)\n",
    "        else:\n",
    "            is_equal = result_eq(flag1[1] , flag2[1] , order_matters=False)\n",
    "        json_dataset['questions'][index+i]['response'] = processed_gen_text\n",
    "        execution_accuracy += is_equal\n",
    "        # print('is_equal: ', is_equal)\n",
    "        # print('--------------------------')\n",
    "    return execution_accuracy\n",
    "\n",
    "from utils.post_process import process_duplication\n",
    "def post_process_get_sql_from_gentext(gen_text):\n",
    "    # remove \\n and extra spaces\n",
    "    sql = \" \".join(gen_text.replace(\"\\n\", \" \").split())\n",
    "    sql = process_duplication(sql)\n",
    "    # python version should >= 3.8\n",
    "    if sql.startswith(\"SELECT\"):\n",
    "        sql = sql\n",
    "    elif sql.startswith(\" \"):\n",
    "        sql = \"SELECT\" + sql\n",
    "    else:\n",
    "        sql = \"SELECT \" + sql\n",
    "    return sql\n",
    "\n",
    "def process_a_batch(question_units, pipeline, pipeline_sequences ,pipeline_index):\n",
    "    start_time = time.time()\n",
    "    batch_list = extract_prompt_from_list_of_questions(question_units)\n",
    "    sequences = generate_answer_from_LLM(batch_list , pipeline)\n",
    "    pipeline_sequences.extend(sequences)\n",
    "    # print(f\"{pipeline_index}: batch_time: \", time.time() - start_time )\n",
    "    return pipeline_sequences\n",
    "    \n",
    "def pipline_process(pipeline , data , batch_size, pipeline_index):\n",
    "    start_time = time.time()\n",
    "    pipeline_sequences = []\n",
    "    chunk_size = len(data)\n",
    "    number_of_batches = int(chunk_size/batch_size)\n",
    "    for index in range( number_of_batches ):\n",
    "        question_units = data[ index*batch_size : index*batch_size+batch_size ]\n",
    "        pipeline_sequences = process_a_batch(question_units, pipeline, pipeline_sequences ,pipeline_index)\n",
    "        if ( index*batch_size + batch_size < chunk_size ) and ( index*batch_size + 2*batch_size > chunk_size ):\n",
    "            question_units = data[index*batch_size + batch_size :]\n",
    "            pipeline_sequences = process_a_batch(question_units, pipeline, pipeline_sequences, pipeline_index)\n",
    "    with open('./log.txt', 'a') as f:\n",
    "        f.write(f\"{pipeline_index}: pipeline_process_time: {time.time() - start_time} \\n\")\n",
    "    # print('pipeline_process_time: ', time.time() - start_time )\n",
    "    return pipeline_sequences\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc11f3d-c76f-4707-bfb8-e72beb0eb57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following ground truth has an error: SELECT  bQuery AS (SELECT DISTINCT T1.atom_id, T1.element, T1.molecule_id, T2.label FROM atom AS T1 INNER JOIN molecule AS T2 ON T1.molecule_id = T2.molecule_id WHERE T2.molecule_id = 'TR006') SELECT CAST(COUNT(CASE WHEN element = 'h' THEN atom_id ELSE NULL END) AS REAL) / (CASE WHEN COUNT(atom_id) = 0 THEN NULL ELSE COUNT(atom_id) END) AS ratio, label FROM SubQuery GROUP BY label\n",
      "The following ground truth has an error: SELECT  xBanned AS (SELECT format, COUNT(*) AS count_banned FROM legalities WHERE status = 'Banned' GROUP BY format ORDER BY COUNT(*) DESC LIMIT 1) SELECT T2.format, T1.name FROM cards AS T1 INNER JOIN legalities AS T2 ON T2.uuid = T1.uuid INNER JOIN MaxBanned MB ON MB.format = T2.format WHERE T2.status = 'Banned'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('./DAIL-SQL/dataset/process/BIRD-TEST_SQL_0-SHOT_CTX-200_ANS-2048/questions.json' , 'r') as f:\n",
    "    generated_prompts_file_byte = f.read()\n",
    "    generated_prompts = json.loads(generated_prompts_file_byte)\n",
    "\n",
    "data_size = len(generated_prompts['questions'])\n",
    "\n",
    "exec_acc = put_responses_back_to_json_dataset(0 , generated_prompts , output_sequences , dataset_type='bird')\n",
    "\n",
    "print('execution accuracy = ' , exec_acc/data_size)\n",
    "with open('./log.txt', 'a') as f:\n",
    "    f.write(f\"execution accuracy = {exec_acc/data_size} \\n\")\n",
    "with open('./llama_pred/BIRD-TEST_SQL_0-SHOT_CTX-200_ANS-2048_Llama_7b_try2.json' , 'w' )as f:\n",
    "    json.dump(generated_prompts , f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3720fc99-50c5-4dc0-b2fd-631de9b71431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9497098646034816\n"
     ]
    }
   ],
   "source": [
    "from utils.post_process import get_exec_output\n",
    "db_dir = './DAIL-SQL/dataset/spider/database'\n",
    "# import asyncio\n",
    "def query_to_db(query , db_id):\n",
    "    db_path = f\"{db_dir}/{db_id}/{db_id}\"\n",
    "    flag, denotation = get_exec_output(\n",
    "            db_path,\n",
    "            query)\n",
    "    return flag, denotation\n",
    "    \n",
    "\n",
    "from utils.post_process import result_eq\n",
    "execution_accuracy = 0\n",
    "counter = 0\n",
    "for q_unit_gen , q_unit_truth in zip(generated_response['questions'] , generated_prompts['questions']):\n",
    "    pred_response = q_unit_gen['response']\n",
    "    start_of_answer = q_unit_truth['prompt'].splitlines()[-1]\n",
    "    ground_truth = start_of_answer + ' ' + q_unit_truth['response']\n",
    "    db_id = q_unit_truth['db_id']\n",
    "    flag1, denotation1 = query_to_db(pred_response , db_id) #flag has ('result' , [data in columns])\n",
    "    flag2, denotation2 = query_to_db(ground_truth , db_id)\n",
    "    if flag1[0] != 'result':\n",
    "        is_equal = False\n",
    "        # print(  counter , '-' , flag1[0] , ' --> ' , 'pred_response: ' , pred_response )\n",
    "    elif 'ORDER BY' in ground_truth or 'order by' in ground_truth:\n",
    "        is_equal = result_eq(flag1[1] , flag2[1] , order_matters=True)\n",
    "    else:\n",
    "        is_equal = result_eq(flag1[1] , flag2[1] , order_matters=False)\n",
    "    execution_accuracy += is_equal\n",
    "    counter += 1\n",
    "print( execution_accuracy/len(generated_response['questions']) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1859d21c-b5eb-4413-b433-86df156843b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print('hello world')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
