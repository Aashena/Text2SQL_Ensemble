{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "25cac957-a95f-4ea7-a560-d7ff63f91623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM , AutoTokenizer\n",
    "import torch.nn.functional as F\n",
    "import os.path\n",
    "import pickle as pkl\n",
    "import json\n",
    "import numpy as np\n",
    "from threading import Thread\n",
    "\n",
    "class LLM_Word_Level_Ensemble:\n",
    "\n",
    "    def __init__(self, model_name , device , tokenizer ):\n",
    "        # Initialize the model and the tokenizer.\n",
    "        self.device = device\n",
    "        # self.model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "        # self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = model_name\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def get_predictions(self, sentences):\n",
    "        # Encode the sentence using the tokenizer and return the model predictions.\n",
    "        self.tokenizer.pad_token = tokenizer.eos_token\n",
    "        self.tokenizer.padding_side=\"left\"\n",
    "        inputs = self.tokenizer(sentences, return_tensors=\"pt\" , padding=True).input_ids.to(self.device)\n",
    "        print('inputs: ' , inputs)\n",
    "        print('type(inputs): ' , type(inputs))\n",
    "        print('inputs.size(): ' , inputs.size())\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(inputs)\n",
    "            predictions = outputs[0]\n",
    "        return predictions\n",
    "\n",
    "    def get_the_top_word_from_probability_vector(self , prob_vector , top_k = 1):\n",
    "        #Gets the top-k words from a probability matrix and returns them.\n",
    "        #Input:\n",
    "            #prob_vector: vector with probability for each word. shape(batch_size , vocab_size)\n",
    "        #Retrun:\n",
    "            #numpy array of size (top_k , batch_size)\n",
    "        \n",
    "        topk_candidates_indexes = torch.topk( prob_vector, top_k ).indices #shape (batch_size , top_k)\n",
    "\n",
    "        print('topk_candidates_indexes: ' , topk_candidates_indexes)\n",
    "        # Decode the top k candidates back to words.\n",
    "        topk_candidates_tokens = tokenizer.batch_decode(topk_candidates_indexes)\n",
    "        if top_k>1:\n",
    "            topk_candidates_seperated_tokens = [words.split() for words in topk_candidates_tokens] #this is for when top_k>1\n",
    "        else:\n",
    "            topk_candidates_seperated_tokens = topk_candidates_tokens\n",
    "        return np.array(topk_candidates_seperated_tokens )\n",
    "        \n",
    "    def get_next_word_probabilities(self, sentences, top_k=-1):#operation_on_a_batch\n",
    "        # Getting the word probabilities for the next word. It will have the probability for the top-k words, and it will zero out the rest. If the top_k argument is set to -1 then it will return the whole probability distribution.\n",
    "        #Input:\n",
    "            #sentences: list of strings. The batch that we want to process.\n",
    "            #top_k: integer. Number of words with the highest probability that we want to have their probability back in the returning vecotr\n",
    "        #Retrun:\n",
    "            #torch tensor. The vector containing the probability of top_k words for the next word. The rest of the words are zeroed out. shape(batch_size , vocab_size)\n",
    "        # Get the model predictions for the sentence.\n",
    "        predictions = self.get_predictions(sentences) #shape [batch_size , sentence_length , vocab_size]\n",
    "        \n",
    "        # Get the next token candidates.\n",
    "        next_token_candidates_tensor = predictions[:, -1, :]\n",
    "        \n",
    "        # Get the token probabilities for all candidates.\n",
    "        all_candidates_probabilities = torch.nn.functional.softmax(\n",
    "            next_token_candidates_tensor, dim=-1)\n",
    "        if top_k<1:\n",
    "            return all_candidates_probabilities\n",
    "            \n",
    "        # Get the top k next token candidates.\n",
    "        topk_candidates_indexes = torch.topk(\n",
    "            next_token_candidates_tensor, top_k).indices #shape( batch_size , top_k )\n",
    "        # topk_candidates_indexes= torch.cat( [topk_candidates_indexes,tokenizer.eos_token_id ],dim=0 )\n",
    "    \n",
    "        batch_size = topk_candidates_indexes.size(dim=0) # This tensor specifies the index of the sentence in the batch for each top-k word\n",
    "        batch_indice_tensor = torch.tensor( list(range( batch_size )) ) # shape (batch_size,)\n",
    "        batch_indice_tensor = torch.reshape(batch_indice_tensor , (batch_size , 1)) #shape(batch_size , 1)\n",
    "        batch_indice_tensor = batch_indice_tensor.expand( batch_size , top_k ) #shape ( batch_size , top_k )\n",
    "\n",
    "        masking_matrix = torch.zeros( ( all_candidates_probabilities.size() ) )\n",
    "        masking_matrix[ batch_indice_tensor , topk_candidates_indexes ] = 1\n",
    "        \n",
    "        # Filter the token probabilities for the top k candidates.\n",
    "        topk_candidates_probabilities = all_candidates_probabilities * masking_matrix\n",
    "        return topk_candidates_probabilities #shape (batch_size , vocab_size)\n",
    "\n",
    "    def convert_flatten_index_to_2D_index(self , flatten_index , array_shape ):\n",
    "        #converting the index in the flatten matrix: datasets_prompts_array.flatten('F')\n",
    "        #input:\n",
    "            #flatten_index: integer. index in the flatten matrix\n",
    "            #array_shape: a tuple. indicating the shape of the 2D array.\n",
    "        #return:\n",
    "            #tuple. indicating the indice in the 2D array\n",
    "        second_dim_index = int(flatten_index/array_shape[0])\n",
    "        first_dim_index = flatten_index%array_shape[0]\n",
    "        return first_dim_index , second_dim_index\n",
    "\n",
    "    # import os.path\n",
    "    # import pickle as pkl\n",
    "    # import json\n",
    "    # import numpy as np\n",
    "    def load_prompts_from_datasets(self , directory , dataset_list , starting_index=0, ending_index=-1):\n",
    "        #getting the prompts from the datasets keeping them in a array accordingly, storing the list of prompts in a file for later use. Getting a numpy array ready for LLM for inference in self.all_prompts_in_1D.\n",
    "        #input:\n",
    "            #dataset_list: list of string. a list of addresses for the json file datasets.\n",
    "            #directory: address of the directory that datasets are in. It ends with '/'\n",
    "            #starting_index: The starting index of prompts that we want to process\n",
    "            #ending_index: The ending index of prompts that we want to process\n",
    "        #return:\n",
    "            #length of the list containing all the prompts\n",
    "        datasets_prompts_list = []\n",
    "        for dataset in dataset_list:\n",
    "            prompt_list = []\n",
    "            potential_cache_file_name = './ensemble_cache/' + dataset+'.pkl'\n",
    "            if os.path.isfile(potential_cache_file_name):\n",
    "                with open(potential_cache_file_name, 'rb') as f:\n",
    "                    prompt_list = pkl.load(f)\n",
    "            else:\n",
    "                dataset_path = directory + dataset + '/' + 'questions.json'\n",
    "                with open( dataset_path , 'r') as f:\n",
    "                    dataset_file_byte = f.read()\n",
    "                    dataset_json_format = json.loads(dataset_file_byte)\n",
    "                question_units = dataset_json_format['questions']\n",
    "                for question_unit in question_units:\n",
    "                    prompt_list.append(question_unit['prompt'])\n",
    "                with open(potential_cache_file_name, 'wb') as f:\n",
    "                    pkl.dump( prompt_list , f )\n",
    "            datasets_prompts_list.append( prompt_list[ starting_index : ending_index ] )\n",
    "        self.datasets_prompts_array = np.array(datasets_prompts_list , dtype='<U10800' ) #shape (number_of_datasset , number_of_prompts)\n",
    "        self.all_prompts_in_1D = self.datasets_prompts_array.flatten('F').tolist()\n",
    "        \n",
    "        self.number_of_components = self.datasets_prompts_array.shape[0]\n",
    "        self.number_of_prompts = self.datasets_prompts_array.shape[1]\n",
    "        self.vocab_size = self.tokenizer.vocab_size\n",
    "        self.last_inferences = torch.zeros( (self.number_of_components , self.number_of_prompts , self.vocab_size ) ) # shape(number_of_components , number_of_prompts , vocab_size)\n",
    "        return len(self.all_prompts_in_1D)\n",
    "\n",
    "    def printPrompt(self , component_index , prompt_index ):\n",
    "        print(self.datasets_prompts_array[component_index,prompt_index])\n",
    "        \n",
    "    def ensemble(self, output_array):\n",
    "        #function for performing ensemble\n",
    "        #input:\n",
    "            #output_array: torch tensor representing the probability of each word in the matrix shape (number_of_components , number_of_ready_indexes_for_ensemble , vocab_size)\n",
    "        #return:\n",
    "            #numpy array containing the new probabilities per word for each prompt. shape( number_of_ready_indexes_for_ensemble , vocab_size )\n",
    "        print('ensemble, output array shape: ' , output_array.size() )\n",
    "        # return torch.mean(output_array , dim=0 )\n",
    "        return output_array[-1,:,:]\n",
    "\n",
    "    \n",
    "        \n",
    "    # import torch.nn.functional as F\n",
    "    def output_handeling(self, batch_index , batch_output):\n",
    "        # This function handels the output of each batch. It performs post process and keeps the data in self.last_inferences so that it updates self.datasets_prompts_array. In the end of each step of inference on the data: \n",
    "        # input:\n",
    "        #     batch_index: index of the first element of the batch in the self.all_prompts_in_1D\n",
    "        #     batch_output: the output of operation_on_a_batch returns. a torch tensor shape ( batch_size , vocab_size )\n",
    "        # return:\n",
    "        #     nothing (The function will affect self.last_inferences)\n",
    "        batch_size = batch_output.size(dim=0)\n",
    "        \n",
    "        #Determining the location of the first element and the last element of the batch_output in the self.last_inference matrix\n",
    "        first_elem_fis_dim_index , first_elem_sec_dim_index = self.convert_flatten_index_to_2D_index(batch_index , self.datasets_prompts_array.shape )\n",
    "        last_elem_fis_dim_index , last_elem_sec_dim_index = self.convert_flatten_index_to_2D_index(batch_index+batch_size-1 , self.datasets_prompts_array.shape )\n",
    "        \n",
    "        #Determining how much padding should be done to the begging and end of the batch_output so that we can use vectorization to update self.last_inference\n",
    "        padding_to_first = first_elem_fis_dim_index\n",
    "        padding_to_last = self.number_of_components - 1 - last_elem_fis_dim_index\n",
    "\n",
    "        #Performing padding on the batch_output and the creating the masking matrix to update self.last_inferences\n",
    "        padded_batch_output = F.pad( input=batch_output, pad=(0, 0, padding_to_first , padding_to_last), mode='constant', value=0) #F.pad(input=source, pad=(0, 1, 1, 1), mode='constant', value=0)\n",
    "        masking_matrix = torch.zeros( ( batch_output.size() ) )\n",
    "        padded_masking_matrix = F.pad( input=masking_matrix, pad=(0, 0, padding_to_first , padding_to_last), mode='constant', value=1) #F.pad(input=source, pad=(0, 1, 1, 1), mode='constant', value=0)\n",
    "\n",
    "        #reshaping and transposing the batch_output and the masking matrix so that they can be applied on self.last_inferences\n",
    "        ready_output_matrix = torch.transpose( torch.reshape( padded_batch_output , (-1, self.number_of_components , self.vocab_size ) ) , dim0=0 , dim1=1).to('cpu')\n",
    "        ready_masking_matrix = torch.transpose( torch.reshape( padded_masking_matrix , (-1, self.number_of_components , self.vocab_size ) )  , dim0=0 , dim1=1)\n",
    "        \n",
    "        #Updating self.last_inferences\n",
    "        self.last_inferences[: , first_elem_sec_dim_index:last_elem_sec_dim_index+1 , :] = self.last_inferences[: , first_elem_sec_dim_index:last_elem_sec_dim_index+1 , :] * ready_masking_matrix\n",
    "        self.last_inferences[: , first_elem_sec_dim_index:last_elem_sec_dim_index+1 , :] = self.last_inferences[: , first_elem_sec_dim_index:last_elem_sec_dim_index+1 , :] + ready_output_matrix\n",
    "\n",
    "        #Determining which indexes in self.last_inferences(dim=1) the are ready for ensemble:\n",
    "        start_of_ready_to_ensemble = first_elem_sec_dim_index\n",
    "        if last_elem_fis_dim_index == self.number_of_components - 1:\n",
    "            end_of_ready_to_ensemble = last_elem_sec_dim_index\n",
    "        else:\n",
    "            end_of_ready_to_ensemble = last_elem_sec_dim_index - 1\n",
    "        \n",
    "        #Now self.last_inferences[: , start_of_ready_to_ensemble:end_of_ready_to_ensemble+1 , :] will be passed so that the ensemble is performed on them\n",
    "        ensembling_output = self.ensemble( self.last_inferences[: , start_of_ready_to_ensemble:end_of_ready_to_ensemble+1 , :] )\n",
    "        new_word_vector = self.get_the_top_word_from_probability_vector( ensembling_output ) # Getting the top word from the ensemble output\n",
    "        print( self.get_the_top_word_from_probability_vector( ensembling_output ,top_k=5 ))\n",
    "        space_vector = np.array( [' '] )\n",
    "        print('new_word_vector:\\n' , new_word_vector)\n",
    "        new_word_vector = np.char.add( space_vector , new_word_vector )\n",
    "        #Making changes to the self.datasets_prompts_array to update the prompts we have\n",
    "        self.new_prompts_array = np.char.add(self.datasets_prompts_array[:,start_of_ready_to_ensemble:end_of_ready_to_ensemble+1], new_word_vector )\n",
    "        self.datasets_prompts_array[:,start_of_ready_to_ensemble:end_of_ready_to_ensemble+1] = self.new_prompts_array\n",
    "            \n",
    "\n",
    "    #from threading import Thread\n",
    "    def pipeline_process(self, batch_size ):\n",
    "        # works on self.all_prompts_in_1D as data. devides the data into batch_sizes and performs the operation on a batch. It handels the output of each operation on a batch\n",
    "        #inputs:\n",
    "            #data: list of prompts\n",
    "            #batch_size: integer indicating the size of a batch.\n",
    "            #operation_on_a_batch: It is a function that is applied on each batch.\n",
    "            #output_handling: a function responsible for handling the output of each operation on a batch.\n",
    "        # Thread_list = []\n",
    "        number_of_iterations = 1\n",
    "        for i in range(number_of_iterations):\n",
    "            data = self.all_prompts_in_1D\n",
    "            data_size = len(data)\n",
    "            number_of_batches = int(data_size/batch_size)\n",
    "            print('iteration_number: ' , i)\n",
    "            for index in range( number_of_batches ):\n",
    "                print('batch_number: ' , index)\n",
    "                batch_index = index*batch_size\n",
    "                if ( batch_index + 2*batch_size > data_size ):\n",
    "                    batch = data[ batch_index : ]\n",
    "                else:\n",
    "                    batch = data[ batch_index : batch_index + batch_size ]\n",
    "                # if Thread_list:\n",
    "                #     Thread_list.pop(0).join()\n",
    "                batch_output = self.get_next_word_probabilities( batch )\n",
    "                self.output_handeling(batch_index , batch_output)\n",
    "                # output_handeling_thread = Thread(target=self.output_handeling , args=(batch_index , batch_output) )\n",
    "                # Thread_list.append(output_handeling_thread)\n",
    "                # output_handeling_thread.start()\n",
    "            self.all_prompts_in_1D = self.datasets_prompts_array.flatten('F').tolist()\n",
    "            \n",
    "        # for i in Thread_list:\n",
    "        #     i.join()\n",
    "\n",
    "\n",
    "# sentence = \"I enjoy walking in the\"\n",
    "# model = LMHeadModel(\"huggyllama/llama-7b\")\n",
    "# model.get_next_word_probabilities(sentence, top_k=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07ef3681-af64-42ec-a63e-2aa3c81b0667",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/yadegari_llama/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:04<00:00,  2.45s/it]\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM , AutoTokenizer\n",
    "\n",
    "# device = 'cpu'\n",
    "device = 'cuda:2'\n",
    "model_name = 'huggyllama/llama-7b'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "84a171b7-ec78-4374-88d7-4b8ae79bff34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_number:  0\n",
      "batch_number:  0\n",
      "inputs:  tensor([[    2,     2,     2,  ...,    13,  6404, 29871],\n",
      "        [    2,     2,     2,  ...,    13,  6404, 29871],\n",
      "        [    1,  4949,  3834,  ...,    13,  6404, 29871]], device='cuda:2')\n",
      "type(inputs):  <class 'torch.Tensor'>\n",
      "inputs.size():  torch.Size([3, 1511])\n",
      "ensemble, output array shape:  torch.Size([3, 1, 32000])\n",
      "topk_candidates_indexes:  tensor([[323]])\n",
      "topk_candidates_indexes:  tensor([[  323, 18134, 29896,  7307,   421]])\n",
      "[['T' 'MAX1' 'CD' '`']]\n",
      "new_word_vector:\n",
      " ['T']\n"
     ]
    }
   ],
   "source": [
    "# sentences = [\"I enjoy walking in the\" , 'What is the whole' , 'Jacke is the worst' , 'Who wants to be a']\n",
    "# sentence = \"I enjoy walking in the dark, and\"\n",
    "# get_next_word_probabilities(sentences, top_k=5)\n",
    "directory = './DAIL-SQL/dataset/process/'\n",
    "dataset1 = 'BIRD-TEST_SQL_0-SHOT_CTX-200_ANS-2048'\n",
    "dataset2 = 'BIRD-TEST_SQL_1-SHOT_EUCDISMASKPRESKLSIMTHR_QA-EXAMPLE_CTX-200_ANS-2048'\n",
    "dataset3 = 'BIRD-TEST_SQL_3-SHOT_EUCDISMASKPRESKLSIMTHR_QA-EXAMPLE_CTX-200_ANS-2048'\n",
    "dataset_list = [dataset1 , dataset2 , dataset3]\n",
    "batch_size = 3 #batch_size should be bigger that the number of componenets in this setting.\n",
    "\n",
    "word_ensemble = LLM_Word_Level_Ensemble(model, device , tokenizer)\n",
    "word_ensemble.load_prompts_from_datasets(directory , dataset_list , starting_index=0, ending_index=1)\n",
    "word_ensemble.pipeline_process( batch_size )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "94d7afb5-a4f3-45e7-aa06-d79e9e5b8e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_number:  0\n",
      "batch_number:  0\n",
      "ensemble, output array shape:  torch.Size([3, 1, 32000])\n",
      "topk_candidates_indexes:  tensor([[323]])\n",
      "topk_candidates_indexes:  tensor([[  323, 29871,    13,   313,  3895]])\n",
      "[['T' '(' 'FROM']]\n",
      "new_word_vector:\n",
      " ['T']\n"
     ]
    }
   ],
   "source": [
    "word_ensemble.pipeline_process( batch_size )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "290ffd91-a564-4822-b44d-f0610a5648de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/* Some SQL examples are provided based on similar problems: */\n",
      "/* Answer the following: What is the highest infant mortality rate per thousand of the countries whose inflation is under 3? */\n",
      "SELECT MAX(T2.Infant_Mortality) FROM economy AS T1 INNER JOIN population AS T2 ON T1.Country = T2.Country WHERE T1.Inflation < 3\n",
      "\n",
      "/* Answer the following: List at least 5 students who has the longest absense from schoool? longest absense refers to MAX(month) */\n",
      "SELECT name FROM longest_absense_from_school ORDER BY month DESC LIMIT 5\n",
      "\n",
      "/* Answer the following: What is the number of unemployed and bankrupt students? */\n",
      "SELECT COUNT(T1.name) FROM unemployed AS T1 INNER JOIN filed_for_bankrupcy AS T2 ON T1.name = T2.name\n",
      "\n",
      "/* Given the following database schema: */\n",
      "CREATE TABLE frpm\n",
      "(\n",
      "    CDSCode                                       TEXT not null\n",
      "        primary key,\n",
      "    `Academic Year`                               TEXT  null,\n",
      "    `County Code`                                 TEXT  null,\n",
      "    `District Code`                               INTEGER         null,\n",
      "    `School Code`                                 TEXT  null,\n",
      "    `County Name`                                 TEXT null,\n",
      "    `District Name`                               TEXT null,\n",
      "    `School Name`                                 TEXT null,\n",
      "    `District Type`                               TEXT null,\n",
      "    `School Type`                                 TEXT null,\n",
      "    `Educational Option Type`                     TEXT null,\n",
      "    `NSLP Provision Status`                       TEXT null,\n",
      "    `Charter School (Y/N)`                        INTEGER    null,\n",
      "    `Charter School Number`                       TEXT  null,\n",
      "    `Charter Funding Type`                        TEXT null,\n",
      "    IRC                                           INTEGER    null,\n",
      "    `Low Grade`                                   TEXT  null,\n",
      "    `High Grade`                                  TEXT null,\n",
      "    `Enrollment (K-12)`                           REAL      null,\n",
      "    `Free Meal Count (K-12)`                      REAL       null,\n",
      "    `Percent (%) Eligible Free (K-12)`            REAL       null,\n",
      "    `FRPM Count (K-12)`                           REAL       null,\n",
      "    `Percent (%) Eligible FRPM (K-12)`            REAL       null,\n",
      "    `Enrollment (Ages 5-17)`                      REAL       null,\n",
      "    `Free Meal Count (Ages 5-17)`                 REAL       null,\n",
      "    `Percent (%) Eligible Free (Ages 5-17)`       REAL       null,\n",
      "    `FRPM Count (Ages 5-17)`                      REAL       null,\n",
      "    `Percent (%) Eligible FRPM (Ages 5-17)`       REAL       null,\n",
      "    `2013-14 CALPADS Fall 1 Certification Status` INTEGER    null,\n",
      "    foreign key (CDSCode) references schools (CDSCode)\n",
      ")\n",
      "\n",
      "CREATE TABLE satscores\n",
      "(\n",
      "    cds         TEXT not null\n",
      "        primary key,\n",
      "    rtype       TEXT  not null,\n",
      "    sname       TEXT null,\n",
      "    dname       TEXT null,\n",
      "    cname       TEXT null,\n",
      "    enroll12    INTEGER         not null,\n",
      "    NumTstTakr  INTEGER          not null,\n",
      "    AvgScrRead  INTEGER          null,\n",
      "    AvgScrMath  INTEGER          null,\n",
      "    AvgScrWrite INTEGER          null,\n",
      "    NumGE1500   INTEGER          null,\n",
      "--     PctGE1500   double      null,\n",
      "        foreign key (cds) references schools (CDSCode)\n",
      ")\n",
      "\n",
      "CREATE TABLE schools\n",
      "(\n",
      "    CDSCode     TEXT not null\n",
      "        primary key,\n",
      "    NCESDist    TEXT  null,\n",
      "    NCESSchool  TEXT  null,\n",
      "    StatusType  TEXT  not null,\n",
      "    County      TEXT not null,\n",
      "    District    TEXT not null,\n",
      "    School      TEXT null,\n",
      "    Street      TEXT null,\n",
      "    StreetAbr   TEXT null,\n",
      "    City        TEXT null,\n",
      "    Zip         TEXT null,\n",
      "    State       TEXT  null,\n",
      "    MailStreet  TEXT null,\n",
      "    MailStrAbr  TEXT null,\n",
      "    MailCity    TEXT null,\n",
      "    MailZip     TEXT null,\n",
      "    MailState   TEXT  null,\n",
      "    Phone       TEXT null,\n",
      "    Ext         TEXT  null,\n",
      "    Website     TEXT null,\n",
      "    OpenDate    DATE        null,\n",
      "    ClosedDate  DATE        null,\n",
      "    Charter     INTEGER    null,\n",
      "    CharterNum  TEXT  null,\n",
      "    FundingType TEXT null,\n",
      "    DOC         TEXT  not null,\n",
      "    DOCType     TEXT not null,\n",
      "    SOC         TEXT  null,\n",
      "    SOCType     TEXT null,\n",
      "    EdOpsCode   TEXT  null,\n",
      "    EdOpsName   TEXT null,\n",
      "    EILCode     TEXT  null,\n",
      "    EILName     TEXT null,\n",
      "    GSoffered   TEXT null,\n",
      "    GSserved    TEXT  null,\n",
      "    Virtual     TEXT  null,\n",
      "    Magnet      INTEGER   null,\n",
      "    Latitude    REAL      null,\n",
      "    Longitude   REAL      null,\n",
      "    AdmFName1   TEXT null,\n",
      "    AdmLName1   TEXT null,\n",
      "    AdmEmail1   TEXT null,\n",
      "    AdmFName2   TEXT null,\n",
      "    AdmLName2   TEXT null,\n",
      "    AdmEmail2   TEXT null,\n",
      "    AdmFName3   TEXT  null,\n",
      "    AdmLName3   TEXT null,\n",
      "    AdmEmail3   TEXT null,\n",
      "    LastUpdate  DATE        not null\n",
      ")\n",
      "\n",
      "/* Answer the following: What is the highest eligible free rate for K-12 students in the schools in Alameda County? Eligible free rate for K-12 = `Free Meal Count (K-12)` / `Enrollment (K-12)` */\n",
      "SELECT  T OP ER T 2 \n",
      " FROM  ( \n",
      " SELECT  T OP ER T\n"
     ]
    }
   ],
   "source": [
    "word_ensemble.printPrompt(2 , 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c4978d3-3ba5-4ffd-8386-9b395afcb582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/* Given the following database schema: */\\nCREATE TABLE frpm\\n(\\n    CDSCode                                       TEXT not null\\n        primary key,\\n    `Academic Year`                               TEXT  null,\\n    `County Code`                                 TEXT  null,\\n    `District Code`                               INTEGER         null,\\n    `School Code`                                 TEXT  null,\\n    `County Name`                                 TEXT null,\\n    `District Name`                               TEXT null,\\n    `School Name`                                 TEXT null,\\n    `District Type`                               TEXT null,\\n    `School Type`                                 TEXT null,\\n    `Educational Option Type`                     TEXT null,\\n    `NSLP Provision Status`                       TEXT null,\\n    `Charter School (Y/N)`                        INTEGER    null,\\n    `Charter School Number`                       TEXT  null,\\n    `Charter Funding Type`                        TEXT null,\\n    IRC                                           INTEGER    null,\\n    `Low Grade`                                   TEXT  null,\\n    `High Grade`                                  TEXT null,\\n    `Enrollment (K-12)`                           REAL      null,\\n    `Free Meal Count (K-12)`                      REAL       null,\\n    `Percent (%) Eligible Free (K-12)`            REAL       null,\\n    `FRPM Count (K-12)`                           REAL       null,\\n    `Percent (%) Eligible FRPM (K-12)`            REAL       null,\\n    `Enrollment (Ages 5-17)`                      REAL       null,\\n    `Free Meal Count (Ages 5-17)`                 REAL       null,\\n    `Percent (%) Eligible Free (Ages 5-17)`       REAL       null,\\n    `FRPM Count (Ages 5-17)`                      REAL       null,\\n    `Percent (%) Eligible FRPM (Ages 5-17)`       REAL       null,\\n    `2013-14 CALPADS Fall 1 Certification Status` INTEGER    null,\\n    foreign key (CDSCode) references schools (CDSCode)\\n)\\n\\nCREATE TABLE satscores\\n(\\n    cds         TEXT not null\\n        primary key,\\n    rtype       TEXT  not null,\\n    sname       TEXT null,\\n    dname       TEXT null,\\n    cname       TEXT null,\\n    enroll12    INTEGER         not null,\\n    NumTstTakr  INTEGER          not null,\\n    AvgScrRead  INTEGER          null,\\n    AvgScrMath  INTEGER          null,\\n    AvgScrWrite INTEGER          null,\\n    NumGE1500   INTEGER          null,\\n--     PctGE1500   double      null,\\n        foreign key (cds) references schools (CDSCode)\\n)\\n\\nCREATE TABLE schools\\n(\\n    CDSCode     TEXT not null\\n        primary key,\\n    NCESDist    TEXT  null,\\n    NCESSchool  TEXT  null,\\n    StatusType  TEXT  not null,\\n    County      TEXT not null,\\n    District    TEXT not null,\\n    School      TEXT null,\\n    Street      TEXT null,\\n    StreetAbr   TEXT null,\\n    City        TEXT null,\\n    Zip         TEXT null,\\n    State       TEXT  null,\\n    MailStreet  TEXT null,\\n    MailStrAbr  TEXT null,\\n    MailCity    TEXT null,\\n    MailZip     TEXT null,\\n    MailState   TEXT  null,\\n    Phone       TEXT null,\\n    Ext         TEXT  null,\\n    Website     TEXT null,\\n    OpenDate    DATE        null,\\n    ClosedDate  DATE        null,\\n    Charter     INTEGER    null,\\n    CharterNum  TEXT  null,\\n    FundingType TEXT null,\\n    DOC         TEXT  not null,\\n    DOCType     TEXT not null,\\n    SOC         TEXT  null,\\n    SOCType     TEXT null,\\n    EdOpsCode   TEXT  null,\\n    EdOpsName   TEXT null,\\n    EILCode     TEXT  null,\\n    EILName     TEXT null,\\n    GSoffered   TEXT null,\\n    GSserved    TEXT  null,\\n    Virtual     TEXT  null,\\n    Magnet      INTEGER   null,\\n    Latitude    REAL      null,\\n    Longitude   REAL      null,\\n    AdmFName1   TEXT null,\\n    AdmLName1   TEXT null,\\n    AdmEmail1   TEXT null,\\n    AdmFName2   TEXT null,\\n    AdmLName2   TEXT null,\\n    AdmEmail2   TEXT null,\\n    AdmFName3   TEXT  null,\\n    AdmLName3   TEXT null,\\n    AdmEmail3   TEXT null,\\n    LastUpdate  DATE        not null\\n)\\n\\n/* Answer the following: What is the highest eligible free rate for K-12 students in the schools in Alameda County? Eligible free rate for K-12 = `Free Meal Count (K-12)` / `Enrollment (K-12)` */\\nSELECT  T'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_ensemble.all_prompts_in_1D[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
