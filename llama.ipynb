{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "983797f7-1536-4310-b157-43dcb5c480ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [01:49<00:00, 54.79s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "# access_token = \"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\"\n",
    "\n",
    "# Specify the device\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Load the model and tokenizer\n",
    "# model_name = \"meta-llama/CodeLlama-7b-hf\"\n",
    "# model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "access_token = \"\"\n",
    "model_name = \"huggyllama/llama-7b\"\n",
    "if access_token=='':\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name , token=access_token)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name , token=access_token).to(device)\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device = device,#\"auto\",\n",
    "    tokenizer=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e43697f4-5319-49c3-9066-591126536e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def extract_prompt_from_list_of_questions(question_list):\n",
    "    batch_list = []\n",
    "    for i in question_list:\n",
    "        batch_list.append(i['prompt'])\n",
    "    return batch_list\n",
    "    \n",
    "def generate_answer_from_LLM(prompt_list , pipeline):\n",
    "    sequences = pipeline(\n",
    "        prompt_list,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        temperature=0.1,\n",
    "        top_p=0.95,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        # eos_token_id=tokenizer.convert_tokens_to_ids(';'),\n",
    "        max_new_tokens=200,\n",
    "    )\n",
    "    return sequences\n",
    "\n",
    "from utils.post_process import get_exec_output\n",
    "db_dir = './DAIL-SQL/dataset/spider/database'\n",
    "# import asyncio\n",
    "def query_to_db(query , db_id):\n",
    "    db_path = f\"{db_dir}/{db_id}/{db_id}\"\n",
    "    flag, denotation = get_exec_output(\n",
    "            db_path,\n",
    "            query)\n",
    "    return flag, denotation\n",
    "\n",
    "from utils.post_process import result_eq\n",
    "def put_responses_back_to_json_dataset(index , json_dataset , sequences):\n",
    "    execution_accuracy = 0\n",
    "    for i , seq in enumerate(sequences):\n",
    "        prompt_len = len ( json_dataset['questions'][index+i]['prompt'] )\n",
    "        qustion = json_dataset['questions'][index+i]['prompt'].splitlines()[-2]\n",
    "        start_of_answer = json_dataset['questions'][index+i]['prompt'].splitlines()[-1]\n",
    "        ground_truth = start_of_answer + ' ' + json_dataset['questions'][index+i]['response']\n",
    "        gen_text = seq[0]['generated_text'][prompt_len:]\n",
    "        processed_gen_text = post_process_get_sql_from_gentext(gen_text)\n",
    "        # print('Question: ' , qustion )\n",
    "        # print('GroundTruth answer: ' ,  ground_truth )\n",
    "        # print('Generated answer: ' , gen_text )\n",
    "        # print('processed_gen_text: ' , processed_gen_text )\n",
    "        db_id = json_dataset['questions'][index+i]['db_id']\n",
    "        flag1, denotation1 = query_to_db(processed_gen_text , db_id) #flag has ('result' , [data in columns])\n",
    "        flag2, denotation2 = query_to_db(ground_truth , db_id)\n",
    "        if flag1[0] != 'result':\n",
    "            is_equal = False\n",
    "            # print(flag1[0] , ' --> ' , 'processed_gen_text: ' , processed_gen_text )\n",
    "        elif 'ORDER BY' in ground_truth or 'order by' in ground_truth:\n",
    "            is_equal = result_eq(flag1[1] , flag2[1] , order_matters=True)\n",
    "        else:\n",
    "            is_equal = result_eq(flag1[1] , flag2[1] , order_matters=False)\n",
    "        json_dataset['questions'][index+i]['response'] = processed_gen_text\n",
    "        execution_accuracy += is_equal\n",
    "        # print('is_equal: ', is_equal)\n",
    "        # print('--------------------------')\n",
    "    return execution_accuracy\n",
    "\n",
    "from utils.post_process import process_duplication\n",
    "def post_process_get_sql_from_gentext(gen_text):\n",
    "    # remove \\n and extra spaces\n",
    "    sql = \" \".join(gen_text.replace(\"\\n\", \" \").split())\n",
    "    sql = process_duplication(sql)\n",
    "    # python version should >= 3.8\n",
    "    if sql.startswith(\"SELECT\"):\n",
    "        sql = sql\n",
    "    elif sql.startswith(\" \"):\n",
    "        sql = \"SELECT\" + sql\n",
    "    else:\n",
    "        sql = \"SELECT \" + sql\n",
    "    return sql\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43c12655-c685-40ca-a7cf-9605a5575615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT S.Name, S.Country, S.Age FROM singer AS S JOIN singer_in_concert AS SIC ON S.Singer_ID = SIC.Singer_ID ORDER BY S.Age ASC \n"
     ]
    }
   ],
   "source": [
    "#Testing the model with a single manually written prompt\n",
    "prompt = '/* Some SQL examples are provided based on similar problems: */ /* Answer the following: Find the name, headquarter and revenue of all manufacturers sorted by their revenue in the descending order. */ SELECT name , headquarter , revenue FROM manufacturers ORDER BY revenue DESC /* Answer the following: Show names of actors in descending order of the year their musical is awarded. */ SELECT T1.Name FROM actor AS T1 JOIN musical AS T2 ON T1.Musical_ID = T2.Musical_ID ORDER BY T2.Year DESC /* Answer the following: List the name and cost of all procedures sorted by the cost from the highest to the lowest. */ SELECT name , cost FROM procedures ORDER BY cost DESC /* Given the following database schema: */ CREATE TABLE \"stadium\" ( \"Stadium_ID\" int, \"Location\" text, \"Name\" text, \"Capacity\" int, \"Highest\" int, \"Lowest\" int, \"Average\" int, PRIMARY KEY (\"Stadium_ID\") ) CREATE TABLE \"singer\" ( \"Singer_ID\" int, \"Name\" text, \"Country\" text, \"Song_Name\" text, \"Song_release_year\" text, \"Age\" int, \"Is_male\" bool, PRIMARY KEY (\"Singer_ID\") ) CREATE TABLE \"concert\" ( \"concert_ID\" int, \"concert_Name\" text, \"Theme\" text, \"Stadium_ID\" text, \"Year\" text, PRIMARY KEY (\"concert_ID\"), FOREIGN KEY (\"Stadium_ID\") REFERENCES \"stadium\"(\"Stadium_ID\") ) CREATE TABLE \"singer_in_concert\" ( \"concert_ID\" int, \"Singer_ID\" text, PRIMARY KEY (\"concert_ID\",\"Singer_ID\"), FOREIGN KEY (\"concert_ID\") REFERENCES \"concert\"(\"concert_ID\"), FOREIGN KEY (\"Singer_ID\") REFERENCES \"singer\"(\"Singer_ID\") ) /* Answer the following: Show name, country, age for all singers ordered by age from the oldest to the youngest. */ SELECT '\n",
    "sequences = generate_answer_from_LLM([prompt] , pipeline)\n",
    "prompt_len = len(prompt)\n",
    "for seq in sequences:\n",
    "    gen_text = seq[0]['generated_text'][prompt_len:]\n",
    "    processed_gen_text = post_process_get_sql_from_gentext(gen_text)\n",
    "    print(processed_gen_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabe8a58-c4cc-450d-a88d-96ecd2c0b7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./DAIL-SQL/dataset/process/SPIDER-TEST_SQL_0-SHOT_CTX-200_ANS-2048/questions.json' , 'r') as f:\n",
    "    generated_prompts_file_byte = f.read()\n",
    "    generated_prompts = json.loads(generated_prompts_file_byte)\n",
    "\n",
    "batch_size = 256\n",
    "exec_acc = 0\n",
    "data_size = len( generated_prompts['questions'] )\n",
    "for index in range( 0 , data_size , batch_size ):\n",
    "    # print(exec_acc)\n",
    "    print('number of processed data: ', index, ' with the accuracy of ' , exec_acc/(index+0.01) )\n",
    "    question_units = generated_prompts['questions'][index:index+batch_size]\n",
    "    batch_list = extract_prompt_from_list_of_questions(question_units)\n",
    "    sequences = generate_answer_from_LLM(batch_list , pipeline)\n",
    "    exec_acc += put_responses_back_to_json_dataset(index , generated_prompts , sequences)\n",
    "        \n",
    "    if ( index + batch_size < len( generated_prompts['questions'] ) ) and ( index + 2*batch_size > len( generated_prompts['questions'] ) ):\n",
    "        question_units = generated_prompts['questions'][index + batch_size :]\n",
    "        batch_list = extract_prompt_from_list_of_questions(question_units)\n",
    "        sequences = generate_answer_from_LLM(batch_list , pipeline)\n",
    "        exec_acc += put_responses_back_to_json_dataset(index , generated_prompts , sequences)\n",
    "    # if (index > 2* batch_size):\n",
    "    #     break\n",
    "print('execution accuracy = ' , exec_acc/data_size)\n",
    "with open('./llama_pred/SPIDER-TEST_SQL_0-SHOT_CTX-200_ANS-2048.json' , 'w' )as f:\n",
    "    json.dump(generated_prompts , f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e2a7fa0-5ac9-42fb-bec6-5ece21b759e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./llama_pred/SPIDER-TEST_SQL_0-SHOT_CTX-200_ANS-2048_CodeLlama7b.json' , 'r') as f:\n",
    "    generated_response_file_byte = f.read()\n",
    "    generated_response = json.loads(generated_response_file_byte)\n",
    "\n",
    "for i in generated_response['questions']:\n",
    "    if i['response'].startswith('SELECT')!=True:\n",
    "        response = i['response']\n",
    "        sql = post_process_get_sql_from_gentext(response)\n",
    "        i['response'] = sql\n",
    "\n",
    "with open('./llama_pred/SPIDER-TEST_SQL_0-SHOT_CTX-200_ANS-2048_CodeLlama7b_postProcessAddedSELECT.json' , 'w' )as f:\n",
    "    json.dump(generated_response , f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d117aa4b-bf9b-406b-8fd9-80e55662f3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution accuracy =  0.25338491295938104\n"
     ]
    }
   ],
   "source": [
    "print('execution accuracy = ' , exec_acc/data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655664c0-c7b9-4ef2-9bc4-579e084a1778",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hello world')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
