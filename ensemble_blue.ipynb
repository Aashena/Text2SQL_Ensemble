{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eada13c1-5557-469a-a827-63c62448c2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/yadegari_llama/lib/python3.12/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/anaconda3/envs/yadegari_llama/lib/python3.12/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/anaconda3/envs/yadegari_llama/lib/python3.12/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json\n",
    "\n",
    "\n",
    "# pred_file_1 = './llama_pred/SPIDER-TEST_SQL_0-SHOT_CTX-200_ANS-2048_Llama_7b.json'\n",
    "# pred_file_2 = './llama_pred/SPIDER-TEST_SQL_1-SHOT_EUCDISQUESTIONMASK_QA-EXAMPLE_CTX-200_ANS-2048_Llama_7b.json'\n",
    "# pred_file_3 = './llama_pred/SPIDER-TEST_SQL_3-SHOT_EUCDISQUESTIONMASK_QA-EXAMPLE_CTX-200_ANS-2048_Llama_7b_try2.json'\n",
    "pred_file_1 = './llama_pred/BIRD-TEST_SQL_0-SHOT_CTX-200_ANS-2048_evidence_Llama_7b.json'\n",
    "pred_file_2 = './llama_pred/BIRD-TEST_SQL_1-SHOT_EUCDISMASKPRESKLSIMTHR_QA-EXAMPLE_CTX-200_ANS-2048_Llama_7b.json'\n",
    "pred_file_3 = './llama_pred/BIRD-TEST_SQL_3-SHOT_EUCDISMASKPRESKLSIMTHR_QA-EXAMPLE_CTX-200_ANS-2048_Llama_7b.json'\n",
    "\n",
    "with open(pred_file_1 , 'r') as f:\n",
    "    refrence_gen_prompt_response_file_byte = f.read()\n",
    "    refrence_gen_prompt_response = json.loads(refrence_gen_prompt_response_file_byte)\n",
    "\n",
    "pred_files = [ pred_file_1 , pred_file_2 , pred_file_3 ]\n",
    "generated_prompt_response_list = []\n",
    "\n",
    "for file in pred_files:\n",
    "    with open(file , 'r') as f:\n",
    "        generated_prompt_response_file_byte = f.read()\n",
    "        generated_prompt_response = json.loads(generated_prompt_response_file_byte)\n",
    "        generated_prompt_response_list.append(generated_prompt_response)\n",
    "\n",
    "for i in range ( len(generated_prompt_response_list[0]['questions']) ): #number of questions we have in a dataset\n",
    "    tokenized_responses = []\n",
    "    for j in range( len( generated_prompt_response_list ) ): #number of ensemble files we have for each question\n",
    "        tokenized_responses.append( word_tokenize( generated_prompt_response_list[j]['questions'][i]['response'] ) )\n",
    "    blue_scores = []\n",
    "    for j in range( len( generated_prompt_response_list ) ):\n",
    "        temp_tokenized_responses = tokenized_responses.copy()\n",
    "        tokenized_response = temp_tokenized_responses.pop(j)\n",
    "        blue_score = sentence_bleu( temp_tokenized_responses , tokenized_response )\n",
    "        blue_scores.append(blue_score)\n",
    "    max_bleu_score_value = max(blue_scores)\n",
    "    max_index_bleu_score = blue_scores.index(max_bleu_score_value)\n",
    "    refrence_gen_prompt_response['questions'][i]['response'] = generated_prompt_response_list[max_index_bleu_score]['questions'][i]['response']\n",
    "    refrence_gen_prompt_response['questions'][i]['prompt'] = generated_prompt_response_list[max_index_bleu_score]['questions'][i]['prompt']\n",
    "with open('./llama_pred/ENSEMBLE_seqLevelVote_BIRD-TEST_SQL_evidence0_1_3-SHOT_EUCDISQUESTIONMASK_QA-EXAMPLE_CTX-200_ANS-2048_Llama_7b.json' , 'w' )as f:\n",
    "    json.dump(refrence_gen_prompt_response , f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec695732-2491-4abd-b08f-259a37a10a32",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'OperationalError' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m     is_equal \u001b[38;5;241m=\u001b[39m result_eq(flag1[\u001b[38;5;241m1\u001b[39m] , flag2[\u001b[38;5;241m1\u001b[39m] , order_matters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m     is_equal \u001b[38;5;241m=\u001b[39m \u001b[43mresult_eq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflag1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflag2\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder_matters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m execution_accuracy \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m is_equal\n\u001b[1;32m     41\u001b[0m counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/utils/post_process.py:80\u001b[0m, in \u001b[0;36mresult_eq\u001b[0;34m(result1, result2, order_matters)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresult_eq\u001b[39m(result1: List[Tuple], result2: List[Tuple], order_matters: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result1) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresult2\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;66;03m# if length is not the same, then they are definitely different bag of rows\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'OperationalError' has no len()"
     ]
    }
   ],
   "source": [
    "from utils.post_process import get_exec_output\n",
    "# db_dir = './DAIL-SQL/dataset/spider/database'\n",
    "db_dir = './DAIL-SQL/dataset/bird/database'\n",
    "# import asyncio\n",
    "def query_to_db(query , db_id):\n",
    "    db_path = f\"{db_dir}/{db_id}/{db_id}\"\n",
    "    flag, denotation = get_exec_output(\n",
    "            db_path,\n",
    "            query)\n",
    "    return flag, denotation\n",
    "    \n",
    "\n",
    "generated_prompts_file = './DAIL-SQL/dataset/process/BIRD-TEST_SQL_0-SHOT_CTX-200_ANS-2048/questions.json'\n",
    "with open(generated_prompts_file , 'r') as f:\n",
    "    generated_prompts_file_byte = f.read()\n",
    "    generated_prompts = json.loads(generated_prompts_file_byte)\n",
    "\n",
    "generated_response_file = './llama_pred/ENSEMBLE_seqLevelVote_BIRD-TEST_SQL_evidence0_1_3-SHOT_EUCDISQUESTIONMASK_QA-EXAMPLE_CTX-200_ANS-2048_Llama_7b.json'\n",
    "with open(generated_response_file , 'r') as f:\n",
    "    generated_response_file_byte = f.read()\n",
    "    generated_response = json.loads(generated_response_file_byte)\n",
    "\n",
    "from utils.post_process import result_eq\n",
    "execution_accuracy = 0\n",
    "counter = 0\n",
    "for q_unit_gen , q_unit_truth in zip(generated_response['questions'] , generated_prompts['questions']):\n",
    "    pred_response = q_unit_gen['response']\n",
    "    start_of_answer = q_unit_truth['prompt'].splitlines()[-1]\n",
    "    ground_truth = start_of_answer + ' ' + q_unit_truth['response']\n",
    "    db_id = q_unit_truth['db_id']\n",
    "    flag1, denotation1 = query_to_db(pred_response , db_id) #flag has ('result' , [data in columns])\n",
    "    flag2, denotation2 = query_to_db(ground_truth , db_id)\n",
    "    if flag1[0] != 'result':\n",
    "        is_equal = False\n",
    "        # print(  counter , '-' , flag1[0] , ' --> ' , 'pred_response: ' , pred_response )\n",
    "    elif 'ORDER BY' in ground_truth or 'order by' in ground_truth:\n",
    "        is_equal = result_eq(flag1[1] , flag2[1] , order_matters=True)\n",
    "    else:\n",
    "        is_equal = result_eq(flag1[1] , flag2[1] , order_matters=False)\n",
    "    execution_accuracy += is_equal\n",
    "    counter += 1\n",
    "print( execution_accuracy/len(generated_response['questions']) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
